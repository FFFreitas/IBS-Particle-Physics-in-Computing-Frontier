{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import glob\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Exploring the Higgs boson with Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Felipe Ferreira de Freitas\n",
    "_______\n",
    "Phys.Rev. D100 (2019) no.3, 035040 (2019-08-31)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"./figs/upgrade_schedule.png\" style=\"width: 700px;\">\n",
    "\n",
    "- <font size=\"5\">Higgs boson detected üëç üëç.</font>\n",
    "- <font size=\"5\">750 GeV üòü üòü.</font>\n",
    "- <font size=\"5\">No SUSY detected üòü üò†.</font>\n",
    "- <font size=\"5\">No DM detected üòü üò† üò†.</font>\n",
    "- <font size=\"5\">No new physics detected üò† üò† üò†, yet.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Overview\n",
    "\n",
    "<font size=\"3\">We  are  shifting  our  focus  from  the resonance searches into more subtle (indirect) effects of new physics.\n",
    "In a nutshell, the SMEFT is a consistent way of exploring new theories as deformations from the SM structures,with a large number of possible SM deviations taken into account.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<font size=\"3\">As an example, in  the  SMEFT  approach  the  Higgs couplings to vector bosons V=W,Z would be modified in the following way:</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<font size=\"5\">$\\eta_{\\mu\\nu} \\, g m_V \\Rightarrow \\eta_{\\mu\\nu} \\, g m_V - \\frac{2 \\, g \\, c_{HW} }{m_W} \\, p^{V}_{\\mu} \\, p^{V}_{\\nu} + \\ldots$</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<font size=\"5\">which in terms of Lagrangian terms would be equivalent to adding to the SM Lagrangian new terms suppressed by a scale of new physics: </br>\n",
    "${\\cal L}_{SM} \\Rightarrow {\\cal L}_{SM} + \\frac{2 i g c_{HW}}{m_W^2} \\, [D^\\mu H^\\dagger T_{2k} D^\\nu H] \\, W^k_{\\mu\\nu} + \\ldots$</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"./figs/w1_HHVV.png\" align=\"center\" style=\"width: 400px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<font size=\"5\">${\\cal L}_{SM} \\Rightarrow {\\cal L}_{SM} + \\frac{2 i g c_{HW}}{m_W^2} \\, [D^\\mu H^\\dagger T_{2k} D^\\nu H] \\, W^k_{\\mu\\nu} + \\ldots$</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<font size=\"3\">One could trace the ultimate origin of these deformations to many different types of new physics, just too heavy to be discovered directly at the LHC. </br> \n",
    "The deformation (aka Wilson coefficient) $c_{HW}$ could be the manifestation of a new set of scalar particles, such as in 2HDMs, too heavy to be seen in direct production, but still felt via virtual effects</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- <font size=\"3\"> As the LHC analyses, in the context of the SMEFT effects in the Higgs sector, moves from the $\\kappa$ formalism to the use of kinematic information, the information contained in multidimensional distributions becomes an important source to identifying even subtler effects.</font>\n",
    "- <font size=\"3\">The need to quickly identify subtle effects in multidimensional distributions of information,clearly calls for artificial intelligence methods.</font>\n",
    "- <font size=\"3\">The  production  of  the Higgs in association with a massive vector boson, or VH, is already firmly established.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Current status: limits on the SMEFT and the VH channel at the LHC: \n",
    "<font size=\"3\">The Wilsoncoefficient $c_{HW}$ it is currently constrained to values in the range (individual constraint):</font>\n",
    "- <font size=\"3\">$c_{HW} = 0.002 \\pm 0.014$</font>\n",
    "\n",
    "<font size=\"3\">The limits on SMEFT operators were obtained by perfoming  a  global  fit  including  kinematic  information  on VH and electroweak WW production at LEP2 and LHC but  only  40  fb$^{-1}$ of  data,  half  of  the  total Run2 dataset.</font>\n",
    "_________\n",
    "<span id=\"#fn1\"> J. Ellis, C. W. Murphy, V. Sanz and T. You, JHEP1806(2018) 146 </span> ,\n",
    "<span id=\"#fn2\"> M. Aaboudet al.[ATLAS Collaboration], Phys. Lett. B786(2018) 59 </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The observation of the Higgs decaying into two b-quarks has been done by combining a challenging set of channels collectively denoted by VH, which corresponds to the Higgs produced in association with a massive vector boson V= Z or $W^{\\pm}$. The final states are classified as 0L ($Z\\rightarrow \\nu \\nu)$, 1L $(W\\rightarrow l \\nu)$ and 2L $(Z\\rightarrow l^{+} l^{-})$.\n",
    "\n",
    "- <font size=\"3\">$\\mu^{\\text{ATLAS}}_{VH} = 1.01 + 0.12 (\\text{stat})^{+0.6}_{0.15}$</font>\n",
    "- <font size=\"3\">naively indicate a two-sigma exclusion for $\\vert c_{HW}\\vert < 0.02$.</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in our analysis:\n",
    "- <font size=\"3\">we select the value $c_{HW} = 0.03$ as a benchmark point.</font>\n",
    "- <font size=\"3\">we consider $c_{HW} \\ne 0$ as our $signal$ and $c_{HW} = 0$ as $backgorund$.</font>\n",
    "- <font size=\"3\">gather the most information as possible from the final state particles.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Kinematic information in VH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<table><tr>\n",
    "    <td><img src='./figs/pthptb1WH.png'></td> \n",
    "    <td><img src='./figs/ptb1ptb2ZH.png'></td>\n",
    "    </tr></table>\n",
    "    \n",
    "- <font size=\"5\">$pp \\longrightarrow Zh, Zh \\longrightarrow \\nu\\bar{\\nu} b\\bar{b}$</font>\n",
    "- <font size=\"5\">$pp \\longrightarrow Wh, Wh \\longrightarrow l\\nu_{l} b\\bar{b}$</font>\n",
    "- <font size=\"5\">$pp \\longrightarrow Zh, Zh \\longrightarrow l^{\\pm}l^{\\mp} b\\bar{b}$</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<font size=\"5\">We consider the following observables as data features:</font>\n",
    "\n",
    "- <font size=\"3\">$p_{T}^{b_{1}}$ transverse momentum of the leading b-jet.</font>\n",
    "- <font size=\"3\">$p_{T}^{b_{2}}$ transverse momentum of the sub leading b-jet.</font>\n",
    "- <font size=\"3\">$p_{T}^{VH}$ transverse momentum of the $VH$ pair.</font>\n",
    "- <font size=\"3\">$M_{T}^{VH}$ transverse mass of the $VH$ pair.</font>\n",
    "- <font size=\"3\">$p_{T}^{W/Z}$ transverse momentum of gauge boson.</font>\n",
    "- <font size=\"3\">$p_{T}^H$ transverse momentum of the reconstructed Higgs boson.</font>\n",
    "- <font size=\"3\">$\\eta^H$ pseudo-rapidity of the reconstructed Higgs boson.</font>\n",
    "- <font size=\"3\">$\\phi^H$ azimuthal angle of the reconstructed Higgs boson.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<font size=\"5\">0L channel:</font>\n",
    "\n",
    "- <font size=\"3\"> $p_{T}^l$ transverse momentum of lepton</font>\n",
    "- <font size=\"3\"> missing transverse energy</font>\n",
    "- <font size=\"3\"> $\\Delta R_{wl}$ separation between lepton and $W$ boson in the $\\eta-\\phi$ plane</font>\n",
    "- <font size=\"3\"> $\\Delta \\phi_{b_1l}$ azimuthal angular separation between leading b-jet and lepton </font>\n",
    "- <font size=\"3\"> $\\Delta \\phi_{l \\text{MET}}$ azimuthal angular separation between lepton and MET </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<font size=\"5\">1L channel:</font>\n",
    "\n",
    "- <font size=\"3\"> $M_{T}^W$ transverse mass of the $W^{\\pm}$</font> \n",
    "- <font size=\"3\">  missing transverse energy </font> \n",
    "- <font size=\"3\"> $\\Delta \\phi_{b_1 \\text{MET}}$ azimuthal angular separation between leading b-jet and MET </font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\">2L channel:</font>\n",
    "\n",
    "- <font size=\"3\"> $p_{T}^{l_1}$ transverse momentum of the leading lepton</font>\n",
    "- <font size=\"3\"> $p_{T}^{l_2}$ transverse momentum of sub-leading lepton</font>\n",
    "- <font size=\"3\"> $\\Delta R_{ll}$ separation between two lepton in the $\\eta-\\phi$ plane</font>\n",
    "- <font size=\"3\"> $\\Delta \\phi_{b_1 l_1}$ azimuthal angular separation between leading b-jet  and leading lepton</font>\n",
    "- <font size=\"3\"> $\\Delta \\phi_{b_2 l_1}$ azimuthal angular separation between sub-leading b-jet  and leading lepton</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How the data looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "list_0L = []\n",
    "for name in sorted(os.listdir('../data/inclusive/'),reverse=True):\n",
    "    filename = '../data/inclusive/' + name\n",
    "    df = pd.read_csv(filename,sep='\\s+',engine='python')\n",
    "    list_0L.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "zh_sm = list_0L[0]\n",
    "zh_chw_0d01_df = list_0L[4]\n",
    "zh_chw_0d03_df = list_0L[3]\n",
    "zh_chw_0d1_df = list_0L[1]\n",
    "zh_chw_1_df = list_0L[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ptb1</th>\n",
       "      <th>ptb2</th>\n",
       "      <th>misset</th>\n",
       "      <th>pth</th>\n",
       "      <th>ptz</th>\n",
       "      <th>etah</th>\n",
       "      <th>phih</th>\n",
       "      <th>mtvh</th>\n",
       "      <th>ptvh</th>\n",
       "      <th>dphib1met</th>\n",
       "      <th>signal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>125.407039</td>\n",
       "      <td>86.117322</td>\n",
       "      <td>205.932526</td>\n",
       "      <td>209.328455</td>\n",
       "      <td>205.932526</td>\n",
       "      <td>1.277426</td>\n",
       "      <td>1.929536</td>\n",
       "      <td>449.712567</td>\n",
       "      <td>4.603307</td>\n",
       "      <td>3.037227</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>131.907616</td>\n",
       "      <td>70.616237</td>\n",
       "      <td>187.837401</td>\n",
       "      <td>190.132443</td>\n",
       "      <td>187.837401</td>\n",
       "      <td>-0.563147</td>\n",
       "      <td>-0.758883</td>\n",
       "      <td>415.367124</td>\n",
       "      <td>3.029103</td>\n",
       "      <td>2.895890</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>142.205381</td>\n",
       "      <td>29.009372</td>\n",
       "      <td>121.233232</td>\n",
       "      <td>120.412595</td>\n",
       "      <td>121.233232</td>\n",
       "      <td>-2.029023</td>\n",
       "      <td>-1.629733</td>\n",
       "      <td>294.761943</td>\n",
       "      <td>1.941833</td>\n",
       "      <td>3.006520</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>106.761004</td>\n",
       "      <td>27.168427</td>\n",
       "      <td>119.788307</td>\n",
       "      <td>120.780482</td>\n",
       "      <td>119.788307</td>\n",
       "      <td>-0.135757</td>\n",
       "      <td>-2.948970</td>\n",
       "      <td>293.614796</td>\n",
       "      <td>0.997513</td>\n",
       "      <td>2.935436</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>94.397035</td>\n",
       "      <td>73.704720</td>\n",
       "      <td>132.123257</td>\n",
       "      <td>134.300456</td>\n",
       "      <td>132.123257</td>\n",
       "      <td>2.286593</td>\n",
       "      <td>-1.262787</td>\n",
       "      <td>315.572440</td>\n",
       "      <td>3.875016</td>\n",
       "      <td>2.604910</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ptb1       ptb2      misset         pth         ptz      etah  \\\n",
       "0  125.407039  86.117322  205.932526  209.328455  205.932526  1.277426   \n",
       "1  131.907616  70.616237  187.837401  190.132443  187.837401 -0.563147   \n",
       "2  142.205381  29.009372  121.233232  120.412595  121.233232 -2.029023   \n",
       "3  106.761004  27.168427  119.788307  120.780482  119.788307 -0.135757   \n",
       "4   94.397035  73.704720  132.123257  134.300456  132.123257  2.286593   \n",
       "\n",
       "       phih        mtvh      ptvh  dphib1met  signal  \n",
       "0  1.929536  449.712567  4.603307   3.037227       1  \n",
       "1 -0.758883  415.367124  3.029103   2.895890       1  \n",
       "2 -1.629733  294.761943  1.941833   3.006520       1  \n",
       "3 -2.948970  293.614796  0.997513   2.935436       1  \n",
       "4 -1.262787  315.572440  3.875016   2.604910       1  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zh_chw_0d01_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The data is not ready yet for the Machine Learning analysis, we have to normalize the entries to be more Machine Learning friendly.\n",
    "- We can use the Scikit-learn library, specifically the MinMaxScaler() function to do this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "def scaleColumns(df, cols_to_scale):\n",
    "    for col in cols_to_scale:\n",
    "        df[col] = pd.DataFrame(min_max_scaler.fit_transform(pd.DataFrame(df[col])),columns=[col])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Before normalize the data, we first need to create our features input $(x)$ and the targets $(y)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cHW_0d01 = pd.concat([zh_chw_0d01_df, zh_sm], ignore_index=True)\n",
    "X_cHW_0d03 = pd.concat([zh_chw_0d03_df, zh_sm], ignore_index=True)\n",
    "X_cHW_0d1 = pd.concat([zh_chw_0d1_df, zh_sm], ignore_index=True)\n",
    "X_cHW_1 = pd.concat([zh_chw_1_df, zh_sm], ignore_index=True)\n",
    "\n",
    "y_cHW_0d01 = X_cHW_0d01['signal']\n",
    "y_cHW_0d03 = X_cHW_0d03['signal']\n",
    "y_cHW_0d1 = X_cHW_0d1['signal']\n",
    "y_cHW_1 = X_cHW_1['signal']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Only normalize your features after combine everything you are going to use. üòâ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [X_cHW_0d01, X_cHW_0d03, X_cHW_0d1, X_cHW_1]:\n",
    "    i = scaleColumns(i,['ptb1', 'ptb2', 'misset', \n",
    "                        'pth', 'ptz', 'etah', \n",
    "                        'phih', 'mtvh', 'ptvh',\n",
    "                        'dphib1met'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### After the normalization, the imputs looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ptb1</th>\n",
       "      <th>ptb2</th>\n",
       "      <th>misset</th>\n",
       "      <th>pth</th>\n",
       "      <th>ptz</th>\n",
       "      <th>etah</th>\n",
       "      <th>phih</th>\n",
       "      <th>mtvh</th>\n",
       "      <th>ptvh</th>\n",
       "      <th>dphib1met</th>\n",
       "      <th>signal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.086453</td>\n",
       "      <td>0.015394</td>\n",
       "      <td>0.069946</td>\n",
       "      <td>0.070658</td>\n",
       "      <td>0.069946</td>\n",
       "      <td>0.317102</td>\n",
       "      <td>0.782922</td>\n",
       "      <td>0.065425</td>\n",
       "      <td>0.520621</td>\n",
       "      <td>0.887508</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.040042</td>\n",
       "      <td>0.079937</td>\n",
       "      <td>0.060259</td>\n",
       "      <td>0.060270</td>\n",
       "      <td>0.060259</td>\n",
       "      <td>0.712868</td>\n",
       "      <td>0.544151</td>\n",
       "      <td>0.055591</td>\n",
       "      <td>0.386925</td>\n",
       "      <td>0.943983</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.188917</td>\n",
       "      <td>0.163342</td>\n",
       "      <td>0.224048</td>\n",
       "      <td>0.223358</td>\n",
       "      <td>0.224048</td>\n",
       "      <td>0.428109</td>\n",
       "      <td>0.001863</td>\n",
       "      <td>0.218536</td>\n",
       "      <td>0.270565</td>\n",
       "      <td>0.975580</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.012409</td>\n",
       "      <td>0.012235</td>\n",
       "      <td>0.008392</td>\n",
       "      <td>0.008747</td>\n",
       "      <td>0.008392</td>\n",
       "      <td>0.288645</td>\n",
       "      <td>0.147635</td>\n",
       "      <td>0.006867</td>\n",
       "      <td>0.141265</td>\n",
       "      <td>0.911109</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.037988</td>\n",
       "      <td>0.037252</td>\n",
       "      <td>0.036305</td>\n",
       "      <td>0.036593</td>\n",
       "      <td>0.036305</td>\n",
       "      <td>0.694466</td>\n",
       "      <td>0.051104</td>\n",
       "      <td>0.032546</td>\n",
       "      <td>0.248878</td>\n",
       "      <td>0.735323</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ptb1      ptb2    misset       pth       ptz      etah      phih  \\\n",
       "0  0.086453  0.015394  0.069946  0.070658  0.069946  0.317102  0.782922   \n",
       "1  0.040042  0.079937  0.060259  0.060270  0.060259  0.712868  0.544151   \n",
       "2  0.188917  0.163342  0.224048  0.223358  0.224048  0.428109  0.001863   \n",
       "3  0.012409  0.012235  0.008392  0.008747  0.008392  0.288645  0.147635   \n",
       "4  0.037988  0.037252  0.036305  0.036593  0.036305  0.694466  0.051104   \n",
       "\n",
       "       mtvh      ptvh  dphib1met  signal  \n",
       "0  0.065425  0.520621   0.887508       1  \n",
       "1  0.055591  0.386925   0.943983       1  \n",
       "2  0.218536  0.270565   0.975580       1  \n",
       "3  0.006867  0.141265   0.911109       1  \n",
       "4  0.032546  0.248878   0.735323       1  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_cHW_1.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ptb1 : 0.06603380945984406 0.07807105493537096\n",
      "ptb2 : 0.05276380572195325 0.06926308503087236\n",
      "misset : 0.06716309934162445 0.08949834993587825\n",
      "pth : 0.06718128565788557 0.08941226283848779\n",
      "ptz : 0.06716309934314747 0.08949834993804219\n",
      "etah : 0.5006507384165415 0.17312476022576348\n",
      "phih : 0.5000665818066831 0.28900209027092005\n",
      "mtvh : 0.06388180888051978 0.08844030270164628\n",
      "ptvh : 0.28225733120272967 0.14680672893989266\n",
      "dphib1met : 0.7684152266915593 0.19672374641410206\n",
      "signal : 0.5 0.5000012500046875\n"
     ]
    }
   ],
   "source": [
    "for i in X_cHW_1.keys():\n",
    "    print(i,':',X_cHW_1[i].mean(),X_cHW_1[i].std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Don't forget to drop the column $signal$ from your features input, otherwise the Machine will imeadiatly know what is $signal$ and what is $BG$ üòâ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [X_cHW_0d01, X_cHW_0d03, X_cHW_0d1, X_cHW_1]:\n",
    "    i.drop(['signal'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((200000, 10), (200000,))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_cHW_0d03.shape, y_cHW_0d03.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Now we can shuffle our inputs and split into training and test datasets:\n",
    "\n",
    "- To avoid pain we can use Scikit-learn function train_test_split(), which automatically take cares of all we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X_train_cHW_0d01, X_test_cHW_0d01, y_train_cHW_0d01, y_test_cHW_0d01 = \\\n",
    "train_test_split(X_cHW_0d01, y_cHW_0d01, train_size=0.8, test_size=0.2)\n",
    "\n",
    "X_train_cHW_0d03, X_test_cHW_0d03, y_train_cHW_0d03, y_test_cHW_0d03 = \\\n",
    "train_test_split(X_cHW_0d03, y_cHW_0d03, train_size=0.8, test_size=0.2)\n",
    "\n",
    "X_train_cHW_0d1, X_test_cHW_0d1, y_train_cHW_0d1, y_test_cHW_0d1 = \\\n",
    "train_test_split(X_cHW_0d1, y_cHW_0d1, train_size=0.8, test_size=0.2)\n",
    "\n",
    "X_train_cHW_1, X_test_cHW_1, y_train_cHW_1, y_test_cHW_1 = \\\n",
    "train_test_split(X_cHW_1, y_cHW_1, train_size=0.8, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- We can save our training and test datasets, so in the future we don't need to do everything again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('../data/inclusive/X_train_0L_cHW_0d03.npz', x=X_train_cHW_0d03, y=y_train_cHW_0d03)\n",
    "np.savez('../data/inclusive/X_train_0L_cHW_0d1.npz', x=X_train_cHW_0d1, y=y_train_cHW_0d1)\n",
    "np.savez('../data/inclusive/X_train_0L_cHW_1.npz', x=X_train_cHW_1, y=y_train_cHW_1)\n",
    "\n",
    "np.savez('../data/inclusive/X_test_0L_cHW_0d03.npz', x=X_test_cHW_0d03, y=y_test_cHW_0d03)\n",
    "np.savez('../data/inclusive/X_test_0L_cHW_0d1.npz', x=X_test_cHW_0d1, y=y_test_cHW_0d1)\n",
    "np.savez('../data/inclusive/X_test_0L_cHW_1.npz', x=X_test_cHW_1, y=y_test_cHW_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## NN architectures: Deep or shalow ü§î"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\">To extract the maximum amount of information from the kinematic features, one  needs to combine multidi-mensional information. The objective is to maximise our ability to detect new phenomena, which in HEP  means  maximising  the  significance  of  an  observation.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Number of layers.\n",
    "- Activation function of each layer.\n",
    "- Regularizers: L1, L2, Batch normalization, drop out.\n",
    "- Drop out probability.\n",
    "- ...\n",
    "\n",
    "<font size=\"3\">To set the best combination of hyper-parameter I built a Evolutionary Algorithm to search through all combinations of parameters and select only the best ones.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Evolutionary search algorithm:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./figs/Basic-evolutionary-algorithm.png\" align=\"center\" style=\"width: 600px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<font size=\"5\">in order to do so we are going to need some very important pieces, the first one is a class to produce random archtectures with the parameters we want to inspect:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class Network():\n",
    "    \"\"\"Represent a network and let us operate on it.\n",
    "    Currently only works for an MLP.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, nn_param_choices=None):\n",
    "        \"\"\"Initialize our network.\n",
    "        Args:\n",
    "            nn_param_choices (dict): Parameters for the network, includes:\n",
    "                nb_neurons (list): [64, 128, 256]\n",
    "                nb_layers (list): [1, 2, 3, 4]\n",
    "                activation (list): ['relu', 'elu']\n",
    "                optimizer (list): ['rmsprop', 'adam']\n",
    "        \"\"\"\n",
    "        self.accuracy = 0.\n",
    "        self.nn_param_choices = nn_param_choices\n",
    "        self.network = {}  # (dic): represents MLP network parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "    def create_random(self):\n",
    "        \"\"\"Create a random network.\"\"\"\n",
    "        for key in self.nn_param_choices:\n",
    "            self.network[key] = random.choice(self.nn_param_choices[key])\n",
    "\n",
    "    def create_set(self, network):\n",
    "        \"\"\"Set network properties.\n",
    "        Args:\n",
    "            network (dict): The network parameters\n",
    "        \"\"\"\n",
    "        self.network = network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def train(self, dataset):\n",
    "        \"\"\"Train the network and record the accuracy.\n",
    "        Args:\n",
    "            dataset (str): Name of dataset to use.\n",
    "        \"\"\"\n",
    "        if self.accuracy == 0.:\n",
    "            self.accuracy = train_and_score(self.network, dataset)\n",
    "\n",
    "    def print_network(self):\n",
    "        \"\"\"Print out a network.\"\"\"\n",
    "        logging.info(self.network)\n",
    "        logging.info(\"Network accuracy: %.2f%%\" % (self.accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<font size=\"5\"> We need a way to evaluate our population of NN, to do so we build an optimizer class: </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer():\n",
    "    \"\"\"Class that implements genetic algorithm for MLP optimization.\"\"\"\n",
    "\n",
    "    def __init__(self, nn_param_choices, retain=0.4,\n",
    "                 random_select=0.1, mutate_chance=0.2):\n",
    "        \"\"\"Create an optimizer.\n",
    "        Args:\n",
    "            nn_param_choices (dict): Possible network paremters\n",
    "            retain (float): Percentage of population to retain after\n",
    "                each generation\n",
    "            random_select (float): Probability of a rejected network\n",
    "                remaining in the population\n",
    "            mutate_chance (float): Probability a network will be\n",
    "                randomly mutated\n",
    "        \"\"\"\n",
    "        self.mutate_chance = mutate_chance\n",
    "        self.random_select = random_select\n",
    "        self.retain = retain\n",
    "        self.nn_param_choices = nn_param_choices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<font size=\"5\"> A function to generate the population:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "    def create_population(self, count):\n",
    "        \"\"\"Create a population of random networks.\n",
    "        Args:\n",
    "            count (int): Number of networks to generate, aka the\n",
    "                size of the population\n",
    "        Returns:\n",
    "            (list): Population of network objects\n",
    "        \"\"\"\n",
    "        pop = []\n",
    "        for _ in range(0, count):\n",
    "            # Create a random network.\n",
    "            network = Network(self.nn_param_choices)\n",
    "            network.create_random()\n",
    "\n",
    "            # Add the network to our population.\n",
    "            pop.append(network)\n",
    "\n",
    "        return pop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<font size=\"5\"> fitness function (a.k.a \"motivation\"):</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "    @staticmethod\n",
    "    def fitness(network):\n",
    "        \"\"\"Return the accuracy, which is our fitness function.\"\"\"\n",
    "        return network.accuracy\n",
    "\n",
    "    def grade(self, pop):\n",
    "        \"\"\"Find average fitness for a population.\n",
    "        Args:\n",
    "            pop (list): The population of networks\n",
    "        Returns:\n",
    "            (float): The average accuracy of the population\n",
    "        \"\"\"\n",
    "        summed = reduce(add, (self.fitness(network) for network in pop))\n",
    "        return summed / float((len(pop)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<font size=\"5\"> Produce new generation of NN üòè </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def breed(self, mother, father):\n",
    "        \"\"\"Make two children as parts of their parents.\n",
    "        Args:\n",
    "            mother (dict): Network parameters\n",
    "            father (dict): Network parameters\n",
    "        Returns:\n",
    "            (list): Two network objects\n",
    "        \"\"\"\n",
    "        children = []\n",
    "        for _ in range(2):\n",
    "\n",
    "            child = {}\n",
    "\n",
    "            # Loop through the parameters and pick params for the kid.\n",
    "            for param in self.nn_param_choices:\n",
    "                child[param] = random.choice(\n",
    "                    [mother.network[param], father.network[param]]\n",
    "                )\n",
    "\n",
    "            # Now create a network object.\n",
    "            network = Network(self.nn_param_choices)\n",
    "            network.create_set(child)\n",
    "\n",
    "            # Randomly mutate some of the children.\n",
    "            if self.mutate_chance > random.random():\n",
    "                network = self.mutate(network)\n",
    "\n",
    "            children.append(network)\n",
    "\n",
    "        return children"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<font size=\"5\"> introducing random mutation <img src=\"./figs/giphy.gif\" align=\"center\" style=\"width: 50px;\"> </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def mutate(self, network):\n",
    "        \"\"\"Randomly mutate one part of the network.\n",
    "        Args:\n",
    "            network (dict): The network parameters to mutate\n",
    "        Returns:\n",
    "            (Network): A randomly mutated network object\n",
    "        \"\"\"\n",
    "        # Choose a random key.\n",
    "        mutation = random.choice(list(self.nn_param_choices.keys()))\n",
    "\n",
    "        # Mutate one of the params.\n",
    "        network.network[mutation] = random.choice(self.nn_param_choices[mutation])\n",
    "\n",
    "        return network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<font size=\"5\"> and evolve our population üß¨ </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def evolve(self, pop):\n",
    "        \"\"\"Evolve a population of networks.\n",
    "        Args:\n",
    "            pop (list): A list of network parameters\n",
    "        Returns:\n",
    "            (list): The evolved population of networks\n",
    "        \"\"\"\n",
    "        # Get scores for each network.\n",
    "        graded = [(self.fitness(network), network) for network in pop]\n",
    "\n",
    "        # Sort on the scores.\n",
    "        graded = [x[1] for x in sorted(graded, key=lambda x: x[0], reverse=True)]\n",
    "\n",
    "        # Get the number we want to keep for the next gen.\n",
    "        retain_length = int(len(graded)*self.retain)\n",
    "\n",
    "        # The parents are every network we want to keep.\n",
    "        parents = graded[:retain_length]\n",
    "\n",
    "        # For those we aren't keeping, randomly keep some anyway.\n",
    "        for individual in graded[retain_length:]:\n",
    "            if self.random_select > random.random():\n",
    "                parents.append(individual)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "        # Now find out how many spots we have left to fill.\n",
    "        parents_length = len(parents)\n",
    "        desired_length = len(pop) - parents_length\n",
    "        children = []\n",
    "\n",
    "        # Add children, which are bred from two remaining networks.\n",
    "        while len(children) < desired_length:\n",
    "\n",
    "            # Get a random mom and dad.\n",
    "            male = random.randint(0, parents_length-1)\n",
    "            female = random.randint(0, parents_length-1)\n",
    "\n",
    "            # Assuming they aren't the same network...\n",
    "            if male != female:\n",
    "                male = parents[male]\n",
    "                female = parents[female]\n",
    "\n",
    "                # Breed them.\n",
    "                babies = self.breed(male, female)\n",
    "\n",
    "                # Add the children one at a time.\n",
    "                for baby in babies:\n",
    "                    # Don't grow larger than desired length.\n",
    "                    if len(children) < desired_length:\n",
    "                        children.append(baby)\n",
    "\n",
    "        parents.extend(children)\n",
    "\n",
    "        return parents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# We can use the Asimov significance as loss function and minimize $(1/Z_{A})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The Asimov significance is defined as follow:\n",
    "\n",
    "<font size=\"5\"> $Z_{A} =\\left[2\\left((s+b)\\ln\\left[\\frac{(s+b)(b+\\sigma_{b}^2)}{b^2+(s+b)\\sigma_{b}^2}\\right] \\right. \\right. \\left. \\left. \\frac{b^2}{\\sigma_{b}^2}\\ln\\left[1+\\frac{\\sigma_{b}^2 s}{b(b+\\sigma_{b}^2)}\\right]\\right)\\right]^{1/2}.$ </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- $s =$ number of signal events\n",
    "- $b =$ number of background events\n",
    "- $\\sigma_{b}^2 =$ systematic uncertanties\n",
    "_________\n",
    "<span id=\"#fn1\"> Adam Elwood, Dirk Kr√ºcker, Direct optimisation of the discovery significance when training neural networks to search for new physics in particle colliders, DESY-18-082 </span> ,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def asimovSignificanceLossInvert(expectedSignal,expectedBkgd,systematic):\n",
    "    '''Define a loss function that calculates the significance based on fixed\n",
    "    expected signal and expected background yields for a given batch size'''\n",
    "\n",
    "\n",
    "    def asimovSigLossInvert(y_true,y_pred):\n",
    "        #Continuous version:\n",
    "\n",
    "        signalWeight=expectedSignal/K.sum(y_true)\n",
    "        bkgdWeight=expectedBkgd/K.sum(1-y_true)\n",
    "\n",
    "        s = signalWeight*K.sum(y_pred*y_true)\n",
    "        b = bkgdWeight*K.sum(y_pred*(1-y_true))\n",
    "        sigB=systematic*b\n",
    "\n",
    "        return 1./(2*((s+b)*K.log((s+b)*(b+sigB*sigB)/(b*b+(s+b)*sigB*sigB+K.epsilon())+K.epsilon())-b*b*K.log(1+sigB*sigB*s/(b*(b+sigB*sigB)+K.epsilon()))/(sigB*sigB+K.epsilon()))) #Add the epsilon to avoid dividing by 0\n",
    "\n",
    "    return asimovSigLossInvert\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## We found that:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1 hidden layer with the same number of neurons as the number of inputs.\n",
    "- ReLu as activation function.\n",
    "- Adam as optimizer.\n",
    "- $L1=0.003$\n",
    "- Batch size of 4096."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Important notes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The Asimov loss takes a little more effort to minimize, so we set a pre-training set of runs for 5 epochs using a steeper loss function.\n",
    "- A longer run, with about 20 ~ 30 epochs is done after the pre-training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "    <td><img src='./figs/pretraining.png'></td> \n",
    "    <td><img src='./figs/higgsBG.png'></td>\n",
    "    </tr></table>\n",
    "    \n",
    "- Distribution of 0L signal (red) and background (blue) events as a function of the classifier output. The left plot is the outcome of performing an initial pretraining run with 5 epochs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table><tr>\n",
    "    <td><img src='./figs/w7_ROCcurves.png'></td> \n",
    "    <td><img src='./figs/truthvsfalse.png'></td>\n",
    "    </tr></table>\n",
    "    \n",
    "- Left: ROC curve for two values of the SMEFT coefficient in the 0L channel. \n",
    "- Right: Classification of true signal events for $c_{HW}=0.03$ and their mapping to kinematic features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# A more interesting way to evaluate our model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "    <td><img src='./figs/rocCurve_train-chw_0d03.png'></td> \n",
    "    <td><img src='./figs/asimovDiscriminatorSyst0p5-chw_0d03.png'></td>\n",
    "    </tr></table>\n",
    "    \n",
    "\n",
    "- $c_{HW} = 0.03$\n",
    "- luminosity: 80 fb$^{-1}$\n",
    "- AUC = 75%\n",
    "- Asimov significance = 31.5, with 50% systematic error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table><tr>\n",
    "    <td><img src='./figs/rocCurve_train_chw_0d001.png'></td> \n",
    "    <td><img src='./figs/asimovDiscriminatorSyst0p5-chw_0d001.png'></td>\n",
    "    </tr></table>\n",
    "    \n",
    "    \n",
    "- $c_{HW} = 0.001$\n",
    "- luminosity: 80 fb$^{-1}$\n",
    "- AUC = 52%\n",
    "- Asimov significance = 1.76, with 50% systematic error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Project the significance to future luminosities:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "    <td><img src='./figs/w2_asimov_vs_lumi_50.png'></td> \n",
    "    <td><img src='./figs/combinedvs0L.png'></td>\n",
    "    </tr></table>\n",
    "    \n",
    "- Left: Luminosity (fb$^{‚àí1}$) versus Asimov significance for different values of $c_{HW}$ in the 0-lepton channel and 50 % systematic uncertainty.\n",
    "- Right: The effect of combination of VH channels for the limiting value $c_{HW}=0.001$ and 50% systematic errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Summarize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Create and deploy a state of art evolutionary algorithm to find the best hyper-parameters in a NN.\n",
    "- Use Asimov significance as loss function prooved to be an important factor in our analysis. \n",
    "- Within  the  framework  of  our  analysis,  we  found  the 0L  channel  to  be  dominant.\n",
    "- We obtained a limit in the SMEFT coefficient $c_{HW}$ of 0.001, about 30 times better than the current constraint from global analysis with  the  Run2  data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Our analysis could be improved in a number of ways:\n",
    "- A more realistic simulation could be performed, including NLO SMEFT effects.\n",
    "- Although we found that deep layers led to overfitting, and a shallow NN was more suitable, new algorithms could be explored to increase sensitivity.\n",
    "- we should understand the effect of switching on more than one wilson coefficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "github.com/FFFreitas/Exploring-SMEFT-in-VH-with-Machine-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Obrigado (Thanks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Backup:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./figs/results_scan_p001.png' align=\"center\" style=\"width: 600px;\">\n",
    "\n",
    "- Classification performance for different linear models tested with ES."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table><tr>\n",
    "    <td><img src='./figs/ZHFbgd.png'></td> \n",
    "    <td><img src='./figs/asimovDiscriminatorSyst0p5_p001_HF.png'></td>\n",
    "    </tr></table>\n",
    "    \n",
    "- $c_{HW} = 0.001$\n",
    "- luminosity: 80 fb$^{-1}$\n",
    "- Vector boson + heavy flavour object as $background$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FastAI",
   "language": "python",
   "name": "fastai_dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
